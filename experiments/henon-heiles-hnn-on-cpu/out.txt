64
-> found 256 CPUs
-> using 64 threads to be consistent with sampling experiments
Step No. : Loss (Squared L2 Error)
-> Loss at step 0	:	104.52862147335276
-> Loss at step 1000	:	23.28030847890558
-> Loss at step 2000	:	13.417828071123186
-> Loss at step 3000	:	6.689170626084589
-> Loss at step 4000	:	2.863741975864974
-> Loss at step 5000	:	1.3666512096803278
-> Loss at step 6000	:	0.8617289160820225
-> Loss at step 7000	:	0.5509454010625887
-> Loss at step 8000	:	0.3747236318375686
-> Loss at step 9000	:	0.18940114841163525
-> Loss at step 10000	:	0.11864987234248997
-> Loss at step 11000	:	0.08866484248292023
-> Loss at step 12000	:	0.06737690230733134
-> Loss at step 13000	:	0.06067941681125441
-> Loss at step 14000	:	0.040093465888782556
-> Loss at step 15000	:	0.026605222661385684
-> Loss at step 16000	:	0.02244157273402947
-> Loss at step 17000	:	0.016905523800704793
-> Loss at step 18000	:	0.015641414462339243
-> Loss at step 19000	:	0.011017771786833424
-> Loss at step 20000	:	0.007574480485758439
-> Loss at step 21000	:	0.007492061840812669
-> Loss at step 22000	:	0.006480956491005253
-> Loss at step 23000	:	0.006648825495141406
-> Loss at step 24000	:	0.005048417569842414
-> Loss at step 25000	:	0.0036673317148540833
-> Loss at step 26000	:	0.0038221176831108325
-> Loss at step 27000	:	0.0034006819564581098
-> Loss at step 28000	:	0.0035628077786670005
-> Loss at step 29000	:	0.002643150216355891
-> Loss at step 30000	:	0.0019771494654712836
-> Loss at step 31000	:	0.0021197232732002138
-> Loss at step 32000	:	0.0019077114776824475
-> Loss at step 33000	:	0.0020714386571083956
-> Loss at step 34000	:	0.0015655031387511265
-> Loss at step 35000	:	0.0012292484119120993
-> Loss at step 36000	:	0.0013432631960952242
-> Loss at step 37000	:	0.0012522285667511727
-> Loss at step 38000	:	0.001443255080513193
-> Loss at step 39000	:	0.0010825290036947403
-> Loss at step 40000	:	0.000871541642029476
-> Loss at step 41000	:	0.0009636923062045616
-> Loss at step 42000	:	0.0009181274187229919
-> Loss at step 43000	:	0.0010735880743482393
-> Loss at step 44000	:	0.0008069724755598813
-> Loss at step 45000	:	0.0006662443391526284
-> Loss at step 46000	:	0.0007340295113456184
-> Loss at step 47000	:	0.0007306066956567002
-> Loss at step 48000	:	0.0008370736997442829
-> Loss at step 49000	:	0.0006239559376499488
-> Loss at step 50000	:	0.0005185006045109076
-> Loss at step 51000	:	0.0005573785823471937
-> Loss at step 52000	:	0.0005627469864450396
-> Loss at step 53000	:	0.0006756127055467824
-> Loss at step 54000	:	0.00048304799842535705
-> Loss at step 55000	:	0.0004058080491863923
-> Loss at step 56000	:	0.00043420806471991394
-> Loss at step 57000	:	0.00044596991299360373
-> Loss at step 58000	:	0.0005463432704965811
-> Loss at step 59000	:	0.0003862302080390385
-> Loss at step 60000	:	0.00032172732621571005
-> Loss at step 61000	:	0.00034573045925895285
-> Loss at step 62000	:	0.00036084618214154984
-> Loss at step 63000	:	0.00047009058442583546
-> Loss at step 64000	:	0.0003149588479215009
-> Loss at step 65000	:	0.0002664275414549953
-> Loss at step 66000	:	0.00028824203753929196
-> Loss at step 67000	:	0.0002948939925648174
-> Loss at step 68000	:	0.00040805700683770957
-> Loss at step 69000	:	0.00026785487362759256
-> Loss at step 70000	:	0.00022635378584222185
-> Loss at step 71000	:	0.0002463677809052259
-> Loss at step 72000	:	0.0002552595929686397
-> Loss at step 73000	:	0.00035780201941263773
-> Loss at step 74000	:	0.0002322381857521726
-> Loss at step 75000	:	0.0001983143190764928
-> Loss at step 76000	:	0.0002165889158777528
-> Loss at step 77000	:	0.00022454325016007547
-> Loss at step 78000	:	0.0003104355095550264
-> Loss at step 79000	:	0.0002087791910452686
-> Loss at step 80000	:	0.00017512419489189307
-> Loss at step 81000	:	0.0001921448771406803
-> Loss at step 82000	:	0.00019807428253887393
-> Loss at step 83000	:	0.0002753541193544829
-> Loss at step 84000	:	0.0001873511126230006
-> Loss at step 85000	:	0.00015542318888015015
-> Loss at step 86000	:	0.0001702816897833479
-> Loss at step 87000	:	0.00017882963403281614
-> Loss at step 88000	:	0.00024954650583798116
-> Loss at step 89000	:	0.00017271993079707163
-> Loss at step 90000	:	0.00013967697902070227
-> Loss at step 91000	:	0.0001542329887890396
-> Loss at step 92000	:	0.00016124299885418093
-> Loss at step 93000	:	0.00022350379775096447
-> Loss at step 94000	:	0.00015430051528915332
-> Loss at step 95000	:	0.00012551892674156709
-> Loss at step 96000	:	0.00014043568656559534
-> Loss at step 97000	:	0.0001443032639242717
-> Loss at step 98000	:	0.00020277205684113037
-> Loss at step 99000	:	0.00013967548598508973
-> Loss at step 100000	:	0.00011797540765571396
-> Loss at step 101000	:	0.00012887691656964396
-> Loss at step 102000	:	0.00013104024739033418
-> Loss at step 103000	:	0.0001846990207700988
-> Loss at step 104000	:	0.00012682682514865762
-> Loss at step 105000	:	0.00010786401178755674
-> Loss at step 106000	:	0.00012096157045382722
-> Loss at step 107000	:	0.00012033270558811797
-> Loss at step 108000	:	0.0001706536957768117
-> Loss at step 109000	:	0.00011710381467193112
-> Loss at step 110000	:	0.0001000774607212852
-> Loss at step 111000	:	0.000114980270293586
-> Loss at step 112000	:	0.00011201831433970862
-> Loss at step 113000	:	0.00015918974532430754
-> Loss at step 114000	:	0.00010975432605987044
-> Loss at step 115000	:	9.489207722058557e-05
-> Loss at step 116000	:	0.00011051150359441423
-> Loss at step 117000	:	0.00010322047798915285
-> Loss at step 118000	:	0.0001442954114153433
-> Loss at step 119000	:	0.00010268316554608643
-> Loss at step 120000	:	8.813522744158375e-05
-> Loss at step 121000	:	0.00010892338946520835
-> Loss at step 122000	:	9.286261845771502e-05
-> Loss at step 123000	:	0.00014221493154725198
-> Loss at step 124000	:	9.367017675623499e-05
-> Loss at step 125000	:	7.889127212473262e-05
-> Loss at step 126000	:	9.370630750992012e-05
-> Loss at step 127000	:	8.559283563743246e-05
-> Loss at step 128000	:	0.0001302539741973541
-> Loss at step 129000	:	8.939098440383833e-05
-> Loss at step 130000	:	7.527423565109895e-05
-> Loss at step 131000	:	8.988509435103935e-05
-> Loss at step 132000	:	7.919497168092822e-05
-> Loss at step 133000	:	0.00013116377386005932
-> Loss at step 134000	:	8.45722869018303e-05
-> Loss at step 135000	:	6.897881573874817e-05
-> Loss at step 136000	:	8.379446696876641e-05
-> Loss at step 137000	:	8.07905878139106e-05
-> Loss at step 138000	:	0.00010250192963105477
-> Loss at step 139000	:	8.060860683897515e-05
-> Loss at step 140000	:	7.001791660288096e-05
-> Loss at step 141000	:	7.363302429365765e-05
-> Loss at step 142000	:	7.650786728441215e-05
-> Loss at step 143000	:	0.00010592614752511788
-> Loss at step 144000	:	7.083166484708582e-05
-> Loss at step 145000	:	6.708504462586064e-05
-> Loss at step 146000	:	7.65666253032242e-05
-> Loss at step 147000	:	6.895584345420484e-05
-> Loss at step 148000	:	0.0001074553573623979
-> Loss at step 149000	:	6.641168305507638e-05
-> Loss at step 150000	:	6.595525293398414e-05
-> Loss at step 151000	:	6.806970713896097e-05
-> Loss at step 152000	:	6.886797766763667e-05
-> Loss at step 153000	:	9.569064406535495e-05
-> Loss at step 154000	:	6.384836724523618e-05
-> Loss at step 155000	:	5.99453735621323e-05
-> Loss at step 156000	:	6.891205079696206e-05
-> Loss at step 157000	:	7.026743213284804e-05
-> Loss at step 158000	:	0.00010099390255913758
-> Loss at step 159000	:	6.312330317890372e-05
-> Loss at step 160000	:	5.896144963416531e-05
-> Loss at step 161000	:	6.839397685933364e-05
-> Loss at step 162000	:	6.266330497640476e-05
-> Loss at step 163000	:	8.14036561972138e-05
-> Loss at step 164000	:	5.840722023953757e-05
-> Loss at step 165000	:	5.897179613695064e-05
-> Loss at step 166000	:	6.0464327458773546e-05
-> Loss at step 167000	:	5.7965242864885326e-05
-> Loss at step 168000	:	9.866906516439943e-05
-> Loss at step 169000	:	5.5929879969564825e-05
-> Loss at step 170000	:	5.409299938260888e-05
-> Loss at step 171000	:	5.606783912541376e-05
-> Loss at step 172000	:	5.565254685632054e-05
-> Loss at step 173000	:	7.20726366400499e-05
-> Loss at step 174000	:	5.5412938094565004e-05
-> Loss at step 175000	:	5.331676588352706e-05
-> Loss at step 176000	:	6.372611962754667e-05
-> Loss at step 177000	:	6.162913319185139e-05
-> Loss at step 178000	:	7.249419931256157e-05
-> Loss at step 179000	:	5.6389266005665627e-05
HNN test (H) error (rel.l2) :  6.68E-04
HNN train time              :  13140.6
-> finished training gradient-descent based training of HNN on cpu, saved under /dss/dsshome1/0B/ge49rev3/shnn-repo/henon-heiles-hnn-on-cpu
