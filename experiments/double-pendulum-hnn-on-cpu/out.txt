64
-> found 256 CPUs
-> using 64 cores to be consistent with other experiments
Step No. : Loss (Squared L2 Error)
-> Loss at step 0	:	0.9852330088615417
-> Loss at step 1000	:	0.07999345660209656
-> Loss at step 2000	:	0.05117452144622803
-> Loss at step 3000	:	0.03914991021156311
-> Loss at step 4000	:	0.03432731702923775
-> Loss at step 5000	:	0.030811646953225136
-> Loss at step 6000	:	0.027195774018764496
-> Loss at step 7000	:	0.023565510287880898
-> Loss at step 8000	:	0.021158751100301743
-> Loss at step 9000	:	0.019670676440000534
-> Loss at step 10000	:	0.018605515360832214
-> Loss at step 11000	:	0.017216360196471214
-> Loss at step 12000	:	0.014757209457457066
-> Loss at step 13000	:	0.013578686863183975
-> Loss at step 14000	:	0.012727073393762112
-> Loss at step 15000	:	0.012054592370986938
-> Loss at step 16000	:	0.011749032884836197
-> Loss at step 17000	:	0.010032149963080883
-> Loss at step 18000	:	0.009442869573831558
-> Loss at step 19000	:	0.008983852341771126
-> Loss at step 20000	:	0.008611897006630898
-> Loss at step 21000	:	0.008845776319503784
-> Loss at step 22000	:	0.007359806913882494
-> Loss at step 23000	:	0.0070790499448776245
-> Loss at step 24000	:	0.00671983789652586
-> Loss at step 25000	:	0.006493690423667431
-> Loss at step 26000	:	0.006880075670778751
-> Loss at step 27000	:	0.005592146888375282
-> Loss at step 28000	:	0.005492186173796654
-> Loss at step 29000	:	0.005277188960462809
-> Loss at step 30000	:	0.005103135481476784
-> Loss at step 31000	:	0.005529718939214945
-> Loss at step 32000	:	0.004456834867596626
-> Loss at step 33000	:	0.004410984925925732
-> Loss at step 34000	:	0.004262901842594147
-> Loss at step 35000	:	0.00409961910918355
-> Loss at step 36000	:	0.004472346976399422
-> Loss at step 37000	:	0.003568510990589857
-> Loss at step 38000	:	0.003538167104125023
-> Loss at step 39000	:	0.003428875235840678
-> Loss at step 40000	:	0.003291657194495201
-> Loss at step 41000	:	0.00358032388612628
-> Loss at step 42000	:	0.002854803344234824
-> Loss at step 43000	:	0.002828296273946762
-> Loss at step 44000	:	0.0027450488414615393
-> Loss at step 45000	:	0.002637333469465375
-> Loss at step 46000	:	0.002878122264519334
-> Loss at step 47000	:	0.002302296692505479
-> Loss at step 48000	:	0.002302148612216115
-> Loss at step 49000	:	0.002252061851322651
-> Loss at step 50000	:	0.002154418732970953
-> Loss at step 51000	:	0.002357280347496271
-> Loss at step 52000	:	0.0019047532696276903
-> Loss at step 53000	:	0.0019115398172289133
-> Loss at step 54000	:	0.0018869652412831783
-> Loss at step 55000	:	0.0017940542893484235
-> Loss at step 56000	:	0.001961488975211978
-> Loss at step 57000	:	0.0016033962601795793
-> Loss at step 58000	:	0.0016169778537005186
-> Loss at step 59000	:	0.0016090166755020618
-> Loss at step 60000	:	0.0015196162275969982
-> Loss at step 61000	:	0.0016579808434471488
-> Loss at step 62000	:	0.0013723565498366952
-> Loss at step 63000	:	0.0013926178216934204
-> Loss at step 64000	:	0.001398649183101952
-> Loss at step 65000	:	0.0013066972605884075
-> Loss at step 66000	:	0.0014250263338908553
-> Loss at step 67000	:	0.001199340564198792
-> Loss at step 68000	:	0.0012166771339252591
-> Loss at step 69000	:	0.0012356513179838657
-> Loss at step 70000	:	0.0011434070765972137
-> Loss at step 71000	:	0.001244490034878254
-> Loss at step 72000	:	0.0010626426665112376
-> Loss at step 73000	:	0.0010832571424543858
-> Loss at step 74000	:	0.0011037233052775264
-> Loss at step 75000	:	0.0010133038740605116
-> Loss at step 76000	:	0.0010985073167830706
-> Loss at step 77000	:	0.0009504834888502955
-> Loss at step 78000	:	0.0009754908387549222
-> Loss at step 79000	:	0.000992775079794228
-> Loss at step 80000	:	0.0009049338405020535
-> Loss at step 81000	:	0.0009758208179846406
-> Loss at step 82000	:	0.0008549471385776997
-> Loss at step 83000	:	0.0008844988187775016
-> Loss at step 84000	:	0.0008963930304162204
-> Loss at step 85000	:	0.0008122734143398702
-> Loss at step 86000	:	0.000870872347149998
-> Loss at step 87000	:	0.0007707161130383611
-> Loss at step 88000	:	0.0008053586352616549
-> Loss at step 89000	:	0.0008104233420453966
-> Loss at step 90000	:	0.0007312099332921207
-> Loss at step 91000	:	0.0007796050049364567
-> Loss at step 92000	:	0.0006950859096832573
-> Loss at step 93000	:	0.0007354692788794637
-> Loss at step 94000	:	0.0007329990621656179
-> Loss at step 95000	:	0.0006595145678147674
-> Loss at step 96000	:	0.0006994635332375765
-> Loss at step 97000	:	0.0006271579768508673
-> Loss at step 98000	:	0.0006737545481882989
-> Loss at step 99000	:	0.0006636054604314268
-> Loss at step 100000	:	0.0005964544252492487
-> Loss at step 101000	:	0.0006291384343057871
-> Loss at step 102000	:	0.0005665677017532289
-> Loss at step 103000	:	0.0006195465684868395
-> Loss at step 104000	:	0.0006006418843753636
-> Loss at step 105000	:	0.0005425370181910694
-> Loss at step 106000	:	0.0005648038350045681
-> Loss at step 107000	:	0.0005123948212713003
-> Loss at step 108000	:	0.0005717413150705397
-> Loss at step 109000	:	0.0005455437931232154
-> Loss at step 110000	:	0.0004955318290740252
-> Loss at step 111000	:	0.0005091022467240691
-> Loss at step 112000	:	0.0004673301591537893
-> Loss at step 113000	:	0.0005287933745421469
-> Loss at step 114000	:	0.0004995219060219824
-> Loss at step 115000	:	0.0004555506457109004
-> Loss at step 116000	:	0.0004626364097930491
-> Loss at step 117000	:	0.0004305676557123661
-> Loss at step 118000	:	0.00048611522652208805
-> Loss at step 119000	:	0.00046008502249605954
-> Loss at step 120000	:	0.00041648384649306536
-> Loss at step 121000	:	0.00041779899038374424
-> Loss at step 122000	:	0.0004030478885397315
-> Loss at step 123000	:	0.0004535309853963554
-> Loss at step 124000	:	0.0004213722422719002
-> Loss at step 125000	:	0.00037628767313435674
-> Loss at step 126000	:	0.0003839038254227489
-> Loss at step 127000	:	0.00037022383185103536
-> Loss at step 128000	:	0.00041989225428551435
-> Loss at step 129000	:	0.0003899444127455354
-> Loss at step 130000	:	0.00034685348509810865
-> Loss at step 131000	:	0.0003528436936903745
-> Loss at step 132000	:	0.00034546575625427067
-> Loss at step 133000	:	0.00039588805520907044
-> Loss at step 134000	:	0.000362907798262313
-> Loss at step 135000	:	0.00032143108546733856
-> Loss at step 136000	:	0.00032593656214885414
-> Loss at step 137000	:	0.0003238647768739611
-> Loss at step 138000	:	0.0003751297772396356
-> Loss at step 139000	:	0.0003394588129594922
-> Loss at step 140000	:	0.0002991221263073385
-> Loss at step 141000	:	0.0003024713369086385
-> Loss at step 142000	:	0.0003049728402402252
-> Loss at step 143000	:	0.00035667116753757
-> Loss at step 144000	:	0.0003187661641277373
-> Loss at step 145000	:	0.0002792349550873041
-> Loss at step 146000	:	0.00028210325399413705
-> Loss at step 147000	:	0.00028863680199719965
-> Loss at step 148000	:	0.0003399309644009918
-> Loss at step 149000	:	0.0002999711432494223
-> Loss at step 150000	:	0.0002613278047647327
-> Loss at step 151000	:	0.0002643020125105977
-> Loss at step 152000	:	0.0002744046214502305
-> Loss at step 153000	:	0.00032488175202161074
-> Loss at step 154000	:	0.00028304391889832914
-> Loss at step 155000	:	0.00024600813048891723
-> Loss at step 156000	:	0.0002480511029716581
-> Loss at step 157000	:	0.0002609229413792491
-> Loss at step 158000	:	0.000311795505695045
-> Loss at step 159000	:	0.0002686564112082124
-> Loss at step 160000	:	0.00023514516942668706
-> Loss at step 161000	:	0.00023486321151722223
-> Loss at step 162000	:	0.00024796652724035084
-> Loss at step 163000	:	0.000299582548905164
-> Loss at step 164000	:	0.00025310387718491256
-> Loss at step 165000	:	0.00022177159553393722
-> Loss at step 166000	:	0.00022304833692032844
-> Loss at step 167000	:	0.00023715774295851588
-> Loss at step 168000	:	0.0002890325849875808
-> Loss at step 169000	:	0.00023882264213170856
-> Loss at step 170000	:	0.00020881550153717399
-> Loss at step 171000	:	0.00021089159417897463
-> Loss at step 172000	:	0.00022769338102079928
-> Loss at step 173000	:	0.0002784036041703075
-> Loss at step 174000	:	0.00022555503528565168
-> Loss at step 175000	:	0.00019713163783308119
-> Loss at step 176000	:	0.00019949954003095627
-> Loss at step 177000	:	0.00021929829381406307
-> Loss at step 178000	:	0.0002685019571799785
-> Loss at step 179000	:	0.0002131315559381619
HNN test (H) error (rel.l2) :  4.09E-03
HNN train time              :  8047.6
-> finished training gradient-descent based training of HNN on cpu, saved under /dss/dsshome1/0B/ge49rev3/shnn-repo/double-pendulum-hnn-on-cpu
