64
-> found 256 CPUs
-> using 64 threads to be consistent with sampling experiments
Step No. : Loss (Squared L2 Error)
-> Loss at step 0	:	0.9355600971146998
-> Loss at step 1000	:	0.08031535087333329
-> Loss at step 2000	:	0.05103087294361686
-> Loss at step 3000	:	0.038929431782049106
-> Loss at step 4000	:	0.03395263834796723
-> Loss at step 5000	:	0.0302281271716887
-> Loss at step 6000	:	0.026724018646937436
-> Loss at step 7000	:	0.02310556840706831
-> Loss at step 8000	:	0.020856566184109658
-> Loss at step 9000	:	0.019271448889522813
-> Loss at step 10000	:	0.018025059811057533
-> Loss at step 11000	:	0.01674396677449958
-> Loss at step 12000	:	0.014251507954021008
-> Loss at step 13000	:	0.013262185036105725
-> Loss at step 14000	:	0.012473451731863205
-> Loss at step 15000	:	0.011789575303846389
-> Loss at step 16000	:	0.011684142264682784
-> Loss at step 17000	:	0.009973511664962204
-> Loss at step 18000	:	0.009550567329696278
-> Loss at step 19000	:	0.00916076613134793
-> Loss at step 20000	:	0.008722184863353345
-> Loss at step 21000	:	0.00905825571747011
-> Loss at step 22000	:	0.00750438026050846
-> Loss at step 23000	:	0.007241080702712964
-> Loss at step 24000	:	0.00685222452820291
-> Loss at step 25000	:	0.006564991791953948
-> Loss at step 26000	:	0.0070179932874631186
-> Loss at step 27000	:	0.005693795818148615
-> Loss at step 28000	:	0.0055905124454027864
-> Loss at step 29000	:	0.005375494193646876
-> Loss at step 30000	:	0.005210862241762183
-> Loss at step 31000	:	0.005716830441086163
-> Loss at step 32000	:	0.004588401208142375
-> Loss at step 33000	:	0.004528763139123841
-> Loss at step 34000	:	0.004402356827177909
-> Loss at step 35000	:	0.004255187155127084
-> Loss at step 36000	:	0.004723174984997344
-> Loss at step 37000	:	0.0037240146114714875
-> Loss at step 38000	:	0.003711482146576674
-> Loss at step 39000	:	0.003617040815976703
-> Loss at step 40000	:	0.003474378984590948
-> Loss at step 41000	:	0.0038503861320174815
-> Loss at step 42000	:	0.0030052841469711163
-> Loss at step 43000	:	0.0030036045448466084
-> Loss at step 44000	:	0.0029418190388597687
-> Loss at step 45000	:	0.002802493726635572
-> Loss at step 46000	:	0.003109268986911793
-> Loss at step 47000	:	0.0024310555708851253
-> Loss at step 48000	:	0.0024525638007107157
-> Loss at step 49000	:	0.0024110574733201334
-> Loss at step 50000	:	0.0022963968042868997
-> Loss at step 51000	:	0.0025295683826636853
-> Loss at step 52000	:	0.0019951363758797637
-> Loss at step 53000	:	0.0020318607648749576
-> Loss at step 54000	:	0.001999824983219838
-> Loss at step 55000	:	0.0019061963121047329
-> Loss at step 56000	:	0.002079211813146572
-> Loss at step 57000	:	0.0016601431574046258
-> Loss at step 58000	:	0.0017054944996333912
-> Loss at step 59000	:	0.0016810985821385537
-> Loss at step 60000	:	0.0016042295952177525
-> Loss at step 61000	:	0.001730063382833782
-> Loss at step 62000	:	0.0014000441829994513
-> Loss at step 63000	:	0.0014554935942118383
-> Loss at step 64000	:	0.0014351430166393083
-> Loss at step 65000	:	0.0013671577688076251
-> Loss at step 66000	:	0.0014635763702722817
-> Loss at step 67000	:	0.0012065519298759316
-> Loss at step 68000	:	0.0012603570218118409
-> Loss at step 69000	:	0.0012448617088395722
-> Loss at step 70000	:	0.0011833975640983656
-> Loss at step 71000	:	0.0012582496302007515
-> Loss at step 72000	:	0.0010573299442275992
-> Loss at step 73000	:	0.0011125887029208945
-> Loss at step 74000	:	0.0010966420491235309
-> Loss at step 75000	:	0.0010390777513679525
-> Loss at step 76000	:	0.0010966663149869086
-> Loss at step 77000	:	0.0009367644042315337
-> Loss at step 78000	:	0.0009943367558323901
-> Loss at step 79000	:	0.0009775406826274514
-> Loss at step 80000	:	0.0009217925011628485
-> Loss at step 81000	:	0.0009655130539049622
-> Loss at step 82000	:	0.0008370408160528346
-> Loss at step 83000	:	0.0008957778170346129
-> Loss at step 84000	:	0.0008778646638403851
-> Loss at step 85000	:	0.0008240349575299026
-> Loss at step 86000	:	0.0008570551845425061
-> Loss at step 87000	:	0.0007526866296417767
-> Loss at step 88000	:	0.0008119192900252216
-> Loss at step 89000	:	0.0007914929072711666
-> Loss at step 90000	:	0.0007408468896628769
-> Loss at step 91000	:	0.0007658995499934017
-> Loss at step 92000	:	0.0006796246768881185
-> Loss at step 93000	:	0.000739682590342395
-> Loss at step 94000	:	0.0007155679850846639
-> Loss at step 95000	:	0.0006691188654989718
-> Loss at step 96000	:	0.0006874986961973305
-> Loss at step 97000	:	0.0006147391116940225
-> Loss at step 98000	:	0.0006763540659104022
-> Loss at step 99000	:	0.000648441877343569
-> Loss at step 100000	:	0.0006082475096635877
-> Loss at step 101000	:	0.0006183267526214166
-> Loss at step 102000	:	0.0005585209571485981
-> Loss at step 103000	:	0.0006216419777836002
-> Loss at step 104000	:	0.0005897687063139561
-> Loss at step 105000	:	0.0005541728916391591
-> Loss at step 106000	:	0.0005589707517759804
-> Loss at step 107000	:	0.0005145506076925984
-> Loss at step 108000	:	0.0005673980823445684
-> Loss at step 109000	:	0.0005383078346476412
-> Loss at step 110000	:	0.0005045233593268398
-> Loss at step 111000	:	0.0005082357003518659
-> Loss at step 112000	:	0.00046871973592156564
-> Loss at step 113000	:	0.0005253439187373487
-> Loss at step 114000	:	0.0004929882385238257
-> Loss at step 115000	:	0.00046213046678695826
-> Loss at step 116000	:	0.0004609128249327125
-> Loss at step 117000	:	0.0004354675666880539
-> Loss at step 118000	:	0.0004943188736141817
-> Loss at step 119000	:	0.0004559148776901533
-> Loss at step 120000	:	0.0004268428459524904
-> Loss at step 121000	:	0.0004239569654373199
-> Loss at step 122000	:	0.0004058162092078425
-> Loss at step 123000	:	0.0004570172941781781
-> Loss at step 124000	:	0.00042170348890168134
-> Loss at step 125000	:	0.0003958839632451141
-> Loss at step 126000	:	0.0003921331269942073
-> Loss at step 127000	:	0.0003801379248972366
-> Loss at step 128000	:	0.00043052115358442405
-> Loss at step 129000	:	0.0003932737847592826
-> Loss at step 130000	:	0.00036974257317725257
-> Loss at step 131000	:	0.00036528959465267593
-> Loss at step 132000	:	0.000356980270985093
-> Loss at step 133000	:	0.0004072559288128242
-> Loss at step 134000	:	0.0003681046314506668
-> Loss at step 135000	:	0.0003469403061365737
-> Loss at step 136000	:	0.0003424229654432789
-> Loss at step 137000	:	0.000336421697085938
-> Loss at step 138000	:	0.00038648833700717026
-> Loss at step 139000	:	0.00034521871047542074
-> Loss at step 140000	:	0.00032604341027879817
-> Loss at step 141000	:	0.000321154098914776
-> Loss at step 142000	:	0.0003188670240490465
-> Loss at step 143000	:	0.0003684985294144684
-> Loss at step 144000	:	0.00032529722684159737
-> Loss at step 145000	:	0.00030587149941130337
-> Loss at step 146000	:	0.00030054626740541493
-> Loss at step 147000	:	0.0003015777237961531
-> Loss at step 148000	:	0.0003533503271554968
-> Loss at step 149000	:	0.0003078251379860467
-> Loss at step 150000	:	0.0002878877955808645
-> Loss at step 151000	:	0.000282160299743799
-> Loss at step 152000	:	0.00028648289709184396
-> Loss at step 153000	:	0.0003373933967399002
-> Loss at step 154000	:	0.00029160769259038884
-> Loss at step 155000	:	0.000273267107124945
-> Loss at step 156000	:	0.00026653326724244004
-> Loss at step 157000	:	0.0002721085083082724
-> Loss at step 158000	:	0.0003228892130218314
-> Loss at step 159000	:	0.0002761787865340957
-> Loss at step 160000	:	0.0002593162014916072
-> Loss at step 161000	:	0.0002525910867786237
-> Loss at step 162000	:	0.00025918588680602834
-> Loss at step 163000	:	0.00030926586372069574
-> Loss at step 164000	:	0.00026150342044164066
-> Loss at step 165000	:	0.0002450813805537533
-> Loss at step 166000	:	0.00023931110877760322
-> Loss at step 167000	:	0.00024824959954902097
-> Loss at step 168000	:	0.00029694017571748157
-> Loss at step 169000	:	0.0002481582101213205
-> Loss at step 170000	:	0.00023217818971812124
-> Loss at step 171000	:	0.00022683585312616457
-> Loss at step 172000	:	0.00023862708223205164
-> Loss at step 173000	:	0.0002857080564610335
-> Loss at step 174000	:	0.00023617822544460464
-> Loss at step 175000	:	0.00022043922298317056
-> Loss at step 176000	:	0.00021519240699348512
-> Loss at step 177000	:	0.00022983143721350406
-> Loss at step 178000	:	0.000275375301191949
-> Loss at step 179000	:	0.0002254389233813287
HNN test (H) error (rel.l2) :  3.62E-03
HNN train time              :  10485.4
-> finished training gradient-descent based training of HNN on cpu, saved under /dss/dsshome1/0B/ge49rev3/shnn-repo/double-pendulum-hnn-on-cpu
